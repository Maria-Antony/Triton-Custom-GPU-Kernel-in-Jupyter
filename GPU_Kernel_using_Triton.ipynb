{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUqORASwLszO",
        "outputId": "204a7e4b-48ec-4f37-b77d-7571c400ead0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install triton torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import time"
      ],
      "metadata": {
        "id": "bO9b779dL2pS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Softmax Kernel Implementation"
      ],
      "metadata": {
        "id": "KYT4j3ziU4Lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "\n",
        "def softmax_kernel(X_ptr, Y_ptr, N, stride_x, stride_y, BLOCK_SIZE: tl.constexpr):\n",
        "    row_id = tl.program_id(0)\n",
        "    offsets = tl.arange(0, BLOCK_SIZE)\n",
        "    row_offsets = row_id * stride_x + offsets\n",
        "\n",
        "    mask = offsets < N\n",
        "    x = tl.load(X_ptr + row_offsets, mask=mask)\n",
        "    x_max = tl.max(x, axis=0)\n",
        "    x = x - x_max\n",
        "    num = tl.exp(x)\n",
        "    denom = tl.sum(num, axis=0)\n",
        "    softmax = num / denom\n",
        "    tl.store(Y_ptr + row_id * stride_y + offsets, softmax, mask=mask)"
      ],
      "metadata": {
        "id": "-amKu0FUMS2t"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B, N = 1024, 4096\n",
        "x = torch.randn((B, N), device='cuda')\n",
        "y_triton = torch.empty_like(x)\n",
        "y_torch = torch.empty_like(x)"
      ],
      "metadata": {
        "id": "2eRqlkXiMaqe"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_triton():\n",
        "    softmax_kernel[(B,)](\n",
        "        x, y_triton, N,\n",
        "        x.stride(0), y_triton.stride(0),\n",
        "        BLOCK_SIZE=N\n",
        "    )"
      ],
      "metadata": {
        "id": "bje7LoQPOybi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_torch():\n",
        "    y_torch.copy_(torch.nn.functional.softmax(x, dim=1))"
      ],
      "metadata": {
        "id": "xN0gAM1vO4Jg"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def time_kernel(fn, *args, repeats=10):\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "    times = []\n",
        "\n",
        "    for _ in range(repeats):\n",
        "        torch.cuda.synchronize()\n",
        "        start.record()\n",
        "        fn(*args)\n",
        "        end.record()\n",
        "        torch.cuda.synchronize()\n",
        "        times.append(start.elapsed_time(end))  # milliseconds\n",
        "\n",
        "    return sum(times) / len(times)"
      ],
      "metadata": {
        "id": "a44oE7XrOsMX"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "triton_ms = time_kernel(run_triton)\n",
        "torch_ms = time_kernel(run_torch)\n",
        "\n",
        "# Accuracy check\n",
        "max_diff = torch.max(torch.abs(y_triton - y_torch)).item()\n",
        "\n",
        "# Print results\n",
        "print(f\"Triton softmax time: {triton_ms:.4f} ms\")\n",
        "print(f\"PyTorch softmax time: {torch_ms:.4f} ms\")\n",
        "print(f\"Max difference: {max_diff:.2e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enBOwPY0NdQi",
        "outputId": "5d8c3083-7f59-41f9-ea73-d9665c0184f7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triton softmax time: 0.2310 ms\n",
            "PyTorch softmax time: 0.4021 ms\n",
            "Max difference: 3.73e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorized Softmax in Triton\n",
        "\n",
        "A softmax kernel where each thread loads 4 floats"
      ],
      "metadata": {
        "id": "zq8WL_IxUjzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "\n",
        "def softmax_kernel_vectorized(\n",
        "    X, Y, stride_xm, stride_ym, n_cols,\n",
        "    BLOCK_SIZE: tl.constexpr, VEC: tl.constexpr\n",
        "):\n",
        "    row = tl.program_id(0)\n",
        "    col_offsets = tl.arange(0, BLOCK_SIZE * VEC)\n",
        "    mask = col_offsets < n_cols\n",
        "\n",
        "    x_ptrs = X + row * stride_xm + col_offsets\n",
        "    y_ptrs = Y + row * stride_ym + col_offsets\n",
        "\n",
        "    x = tl.load(x_ptrs, mask=mask, other=-float(\"inf\"))\n",
        "    row_max = tl.max(x, axis=0)\n",
        "    x = x - row_max\n",
        "    exp_x = tl.exp(x)\n",
        "    row_sum = tl.sum(exp_x, axis=0)\n",
        "    result = exp_x / row_sum\n",
        "    tl.store(y_ptrs, result, mask=mask)"
      ],
      "metadata": {
        "id": "lNfuPOYqc3UQ"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BLOCK_SIZE = 128\n",
        "VEC = 4  # each thread processes 4 values\n",
        "\n",
        "def run_triton_vectorized():\n",
        "  softmax_kernel_vectorized[(B,)](\n",
        "      x, y_triton, x.stride(0), y_triton.stride(0), N,\n",
        "      BLOCK_SIZE=BLOCK_SIZE,\n",
        "      VEC=VEC\n",
        "  )"
      ],
      "metadata": {
        "id": "a6L13aDfWLbS"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B, N = 1024, 4096\n",
        "x = torch.randn((B, N), device='cuda')\n",
        "y_triton_naive = torch.empty_like(x)\n",
        "y_triton_vec = torch.empty_like(x)\n",
        "y_torch = torch.empty_like(x)\n",
        "\n",
        "# Run benchmarks\n",
        "time_naive = time_kernel(run_triton)\n",
        "time_vec = time_kernel(run_triton_vectorized)\n",
        "time_torch = time_kernel(run_torch)\n",
        "\n",
        "# Accuracy check\n",
        "diff_naive = torch.max(torch.abs(y_triton_naive - y_torch)).item()\n",
        "diff_vec = torch.max(torch.abs(y_triton_vec - y_torch)).item()\n",
        "\n",
        "print(f\"Max diff (naive): {diff_naive:.2e}\")\n",
        "print(f\"Max diff (vectorized): {diff_vec:.2e}\")\n",
        "print(\"--------------------------------------\")\n",
        "\n",
        "print(f\"Triton naive softmax time: {time_naive:.4f} ms\")\n",
        "print(f\"Triton vectorized softmax time: {time_vec:.4f} ms\")\n",
        "print(f\"PyTorch softmax time: {time_torch:.2e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2_kjDL8WLYx",
        "outputId": "84e3dc8a-2165-46ea-d9d0-8ca605c39387"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max diff (naive): 2.48e-02\n",
            "Max diff (vectorized): 5.01e+00\n",
            "--------------------------------------\n",
            "Triton naive softmax time: 0.2953 ms\n",
            "Triton vectorized softmax time: 0.0655 ms\n",
            "PyTorch softmax time: 4.00e-01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### This kernel implements a tiled and vectorized softmax using Triton. Since the input rows can be very large, the row is split into multiple tiles, each processed in chunks of BLOCK_SIZE Ã— VEC. The kernel runs in three passes: first to compute the global row-wise max (for numerical stability), second to compute the sum of exponentials, and third to normalize and store the final softmax values. The use of vectorized memory access improves performance by loading multiple values at once, while tiling ensures the kernel scales to wide input sizes efficiently."
      ],
      "metadata": {
        "id": "5Y11LOC3Wcm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def softmax_kernel_tiled(\n",
        "    X, Y, stride_xm, stride_ym, n_cols,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "    VEC: tl.constexpr\n",
        "):\n",
        "    row = tl.program_id(0)\n",
        "    col_offsets = tl.arange(0, BLOCK_SIZE * VEC)  # vectorized offsets\n",
        "    row_max = -float(\"inf\")\n",
        "    row_sum = 0.0\n",
        "\n",
        "    # === Pass 1: Row-wise max\n",
        "    for i in range(0, n_cols, BLOCK_SIZE * VEC):\n",
        "        cols = col_offsets + i\n",
        "        mask = cols < n_cols\n",
        "        x_ptrs = X + row * stride_xm + cols\n",
        "        x = tl.load(x_ptrs, mask=mask, other=-float(\"inf\"))\n",
        "        row_max = tl.maximum(row_max, tl.max(x, axis=0))\n",
        "\n",
        "    # === Pass 2: Sum of exp(x - max)\n",
        "    for i in range(0, n_cols, BLOCK_SIZE * VEC):\n",
        "        cols = col_offsets + i\n",
        "        mask = cols < n_cols\n",
        "        x_ptrs = X + row * stride_xm + cols\n",
        "        x = tl.load(x_ptrs, mask=mask, other=-float(\"inf\"))\n",
        "        exp_x = tl.exp(x - row_max)\n",
        "        row_sum += tl.sum(exp_x, axis=0)\n",
        "\n",
        "    # === Pass 3: Normalize and store\n",
        "    for i in range(0, n_cols, BLOCK_SIZE * VEC):\n",
        "        cols = col_offsets + i\n",
        "        mask = cols < n_cols\n",
        "        x_ptrs = X + row * stride_xm + cols\n",
        "        y_ptrs = Y + row * stride_ym + cols\n",
        "        x = tl.load(x_ptrs, mask=mask, other=-float(\"inf\"))\n",
        "        exp_x = tl.exp(x - row_max)\n",
        "        softmax = exp_x / row_sum\n",
        "        tl.store(y_ptrs, softmax, mask=mask)\n"
      ],
      "metadata": {
        "id": "4kEEJHVBdDkG"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BLOCK_SIZE = 128\n",
        "VEC = 4\n",
        "\n",
        "def run_triton_tiled():\n",
        "    softmax_kernel_tiled[(B,)](\n",
        "      x, y_triton, x.stride(0), y_triton.stride(0), N,\n",
        "      BLOCK_SIZE=BLOCK_SIZE,\n",
        "      VEC=VEC\n",
        "  )"
      ],
      "metadata": {
        "id": "pvo4WEV3UL1s"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B, N = 1024, 4096\n",
        "x = torch.randn((B, N), device='cuda')\n",
        "y_triton_naive = torch.empty_like(x)\n",
        "y_triton_vec = torch.empty_like(x)\n",
        "y_triton_tiled = torch.empty_like(x)\n",
        "y_torch = torch.empty_like(x)\n",
        "\n",
        "# Run benchmarks\n",
        "time_naive = time_kernel(run_triton)\n",
        "time_vec = time_kernel(run_triton_vectorized)\n",
        "time_tiled = time_kernel(run_triton_tiled)\n",
        "time_torch = time_kernel(run_torch)\n",
        "\n",
        "# Accuracy check\n",
        "diff_naive = torch.max(torch.abs(y_triton_naive - y_torch)).item()\n",
        "diff_vec = torch.max(torch.abs(y_triton_vec - y_torch)).item()\n",
        "diff_tiled = torch.max(torch.abs(y_triton_tiled - y_torch)).item()\n",
        "\n",
        "print(f\"Max diff (naive): {diff_naive:.2e}\")\n",
        "print(f\"Max diff (vectorized): {diff_vec:.2e}\")\n",
        "print(f\"Max diff (vectorized + tiled): {diff_tiled:.2e}\")\n",
        "print(\"--------------------------------------\")\n",
        "\n",
        "print(f\"Triton softmax time: {time_naive:.4f} ms\")\n",
        "print(f\"Triton vectorized softmax time: {time_vec:.4f} ms\")\n",
        "print(f\"Triton vectorized + tiled softmax time: {time_tiled:.4f} ms\")\n",
        "print(f\"PyTorch softmax time: {time_torch:.4f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5vdiCmrSpoI",
        "outputId": "039159ac-cd0a-4af3-a66b-932c4451c864"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max diff (naive): 2.19e-02\n",
            "Max diff (vectorized): 5.24e+00\n",
            "Max diff (vectorized + tiled): 2.15e-02\n",
            "--------------------------------------\n",
            "Triton softmax time: 0.4511 ms\n",
            "Triton vectorized softmax time: 0.1155 ms\n",
            "Triton vectorized + tiled softmax time: 0.3728 ms\n",
            "PyTorch softmax time: 0.4187 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iJrKnu7NY2nf"
      },
      "execution_count": 57,
      "outputs": []
    }
  ]
}